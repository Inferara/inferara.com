# **A Hammer in Search of Nails**

In the process of developing a new tool, a reasonable need inevitably arises at some point to test it outside of one's own workshop, in the uncontrolled environment of the real world. Our innovative framework for specifying and verifying programs known as **Inference** is reaching exactly this stage. Calling us as its creators to try this futuristic hammer on every nail they encounter. However, since there are only 24 hours in a day and the number of projects on GitHub to which formal methods could theoretically be applied is substantially larger than makes sense to count, one has to somehow filter the potential territory of work.

For obvious reasons, when discussing ways to improve software reliability, it is sensible to focus on projects with a higher cost of error. Blockchain platforms inevitably end up at the top of the list because damage from emergency situations in the crypto industry almost always has a direct monetary valuation, the average value of which continues to grow. A second natural selection criterion is the technology stack: the automation of the WebAssembly standard became the experimental testing ground for our technology. Therefore, a short list of nails suitable for our hammer at the current stage can be composed of blockchain platforms using WebAssembly as their primary compilation target. Within this article, we will touch upon three such projects: Polkadot Substrate, Stellar Soroban, and Arbitrum Stylus.

When looking at this trio from a distance, a certain superficial kinship catches the eye as they all use Rust as one of the primary languages for contract development, and the WebAssembly platform is the foundation of their runtime. However, upon closer examination through the lens of formal verifiability, fundamental architectural differences begin to manifest, which I would like to discuss here.

# **Polkadot Substrate**

<!-- Insert polkadot log or something here -->

The framework authored by the Web3 Foundation represents a continuously expanding collection of ready-made solutions for core crypto-financial accounting patterns, organized as a collection of modules with a special structure (pallets) in the Rust language. A high-tech build system allows these modules to be combined in any way, thereby producing new versions of the runtime containing only the functionality necessary for each individual case. The main architectural principles of the collection can be formulated as follows:

1. The pallet as a universal unit of integration. Any functionality that a blockchain developer might need, from individual aspects of infrastructure to popular business logic patterns, is provided in the form of uniformly connected modules.  
2. Active use of the Rust type system and advanced metaprogramming tools in the horizontal interaction of modules for the most comfortable and safe code reuse.  
3. Great attention to runtime performance optimization using the most powerful techniques. Aggressive inlining at the Link Time Optimization (LTO) stage is the default setting when building the final binary.

At first, it might seem that the qualities listed above are a hymn of praise, but unfortunately, such an architecture also has a dark side that inevitably follows from the same combination of factors that create its advantages. In the shadow of these benefits, alas, hide problems capable of effectively putting an end to the feasibility of an entire class of methods for formally ensuring the reliability of the system both as a whole and in parts.

When discussing the methodological benchmark of formal methods, which is the verification of executable binary modules at the level of platform semantics, any sufficiently large codebase subject to verification requires a series of properties that programmers usually rarely worry about. These requirements arise in connection with the fundamental principles of reasoning about algorithms and are, in fact, mandatory:

<!--  might be a good place for images here -->

1. Complex systems are not verifiable in their entirety. The only practical way to verify complex systems consists of breaking them down and specifying subsystems that are as isolated from each other as possible, which are then verified independently of each other.  
2. The total costs of verification grow proportionally to the square of the number of subsystems sharing a common state space. Without clearly delineated boundaries of "ownership" that are ensured at the platform level, verification must proceed from the presumption of mutual influence of everyone on everyone.  
3. Reusing judgments about algorithm properties is much harder than reusing the algorithms themselves. Monomorphization and inlining are effectively free for the developer and end user but are very costly in the context of formal verification because every copy of the code produced by these procedures becomes a separate target requiring independent coverage.

In this light, it suddenly turns out that the FRAME architecture unfortunately chooses to complicate the task of formal verification at almost every turn. Yes, at the source code level, the pallet system radiates flexibility, modularity, and ease of use. However, full-scale formal verification of Rust source code will remain a utopian dream for a long time simply due to the lack of an official specification of its semantics, while at the level of executable binaries, the picture becomes much more dismal.

The central problem is that all structural boundaries between source code modules are completely erased by the assembly process at a fairly early stage. In fact, the ability to use macro definitions from one pallet in another erodes them even at the preprocessing stage. If one disassembles the final result, for example the official Polkadot runtime, the result will generally represent one monolithic module with over two million instructions, where for very rare functions one can uniquely establish their membership to any specific pallet. Worse yet, at the center of this artifact will lie one function with several hundred thousand lines, generated by the automatic inlining of a myriad of completely heterogeneous pieces of code from all corners of the project directly into the body of the central event dispatcher.

Alas, the only thing any formal verification engineer can do when faced with such a front of work is to throw up their hands and give up, since at the moment specifying and proving correctness in such conditions is technologically nearly impossible and economically simply absurd. Even if by some miracle one could create a comprehensive and strictly proven specification of a specific version of the Polkadot runtime, this achievement would be completely non-transferable not only to its derivative forks with a different composition of included pallets but even to subsequent versions of Polkadot itself. Furthermore, considering the lack of reliable determinism in the Rust build process even when sandboxing with docker, it will sometimes be enough to simply rebuild the binary on another machine to invalidate it. Due to thread contention (apparently) during compilation parallelization, the shared memory address space and the ordinal indexing of functions are shuffled completely freely.

Is it possible to somehow compensate for the problems listed above with little effort without performing a radical review of the architecture? This is a difficult question considering how rapidly progress moves in this area. It is possible that in just a few years, verification automation using LLMs will allow us to simply throw a rack of graphics cards at all the scale problems. But until then, I fear the forecast is disappointing: attempts to obtain formal confirmation of the correctness of any runtime inheriting the Polkadot codebase in the form in which it is deployed and executed appear impractical. This should not, however, deter its users and developers from applying formal methods to increase reliability on a smaller scale. For example, non-trivial arithmetic in important functions can be verified at the pseudocode level, and the isolation of contracts in separate wasm modules allows for the consideration of assembly-level verification for those at least.

# **Stellar Soroban**

When encountering this platform after Substrate, the first thing that catches the eye is radical minimalism. The dedicated core of the cryptographic ledger contains only 500 thousand lines of a very orthodox subset of C/C++. There are no excesses, only the basic functionality of the network protocol and the algorithm implementing distributed consensus. There are no optional components, as every function is necessary for the system to work. Metaprogramming and polymorphism are almost never used, and classic procedural code operates straightforwardly on explicitly defined data structures. Our grandfathers and great-grandfathers wrote code this way, you might joke, and you would not be far from the truth. Indeed, with adjustments for the progress of the TCP/IP stack and a small artistic exaggeration, the Stellar core could be built for a mainframe of the last millennium using a compiler from the mid-90s.

However, it would be a great mistake to mistake this minimalism for being archaic. Some development methodologies change little over the years for the same reason crocodiles do not evolve, which is due to perfect adaptation to their ecological niche. As paradoxical as it may sound, if you are planning a system subject to formal verification at the level of the executable binary, the classic procedural style of straightforwardly operating on explicitly defined data structures is your best and, with the growth of system size, your only chance.

The fact is that this seemingly archaic development method is optimized from the point of view of the main parameter affecting the verifiability of the system as a whole, which is the structural distance between the source code and the assembly listing. By consciously giving up the comfort and flexibility familiar to the modern programmer, we receive in exchange an invaluable advantage for reasoning about algorithms. This is the ability to practically perform a line-by-line comparison of language operators and expressions used by the programmer with compact groups of assembly instructions obtained at the output of the assembly process. The clarity of the developer's thought, which is completely lost when grinding additional layers of abstraction through the meat grinder of modern languages, remains relatively intact through the transparent compilation process of orthodox C/C++ and can be read by a trained eye, often even after applying fairly aggressive optimization techniques.

At the same time, Stellar developers do not force end users into the same harsh asceticism when developing Soroban contracts deployed on the blockchain. Yes, when communicating with the external environment such as the platform and other contracts, the programmer has to limit themselves to a classic monomorphic FFI with a small list of permitted base types, which from a verification point of view is again a significant plus. However, the official SDK is written in Rust, so inside the contract itself, everyone is free to build arbitrarily complex abstractions without significant obstacles. At the same time, thanks to the isolated WebAssembly execution environment and the lack of shared state with the infrastructure and other contracts, the programmer does not have to worry about destroying any reliability guarantees provided by the platform.

Directly comparing the Polkadot and Stellar platforms is not very fair because the former, in addition to basic blockchain infrastructure, offers the user a rich assortment of ready-made engineering solutions for all occasions, while the latter explicitly disclaims responsibility for all business logic issues. Usually, functionality that is a single inclusion of a ready-made pallet away in Substrate is expected to be implemented by the user themselves in Soroban in the form of contracts based on examples from a separate repository. However, it is still difficult not to notice how strongly architectural decisions affect the prospects of formal verification. Yes, 500k lines of C/C++ is also quite a lot, and full verification of the project would likely stand alongside unique achievements like seL4 and CompCert, but the success of implementing this undertaking can at least be imagined\!

# **Arbitrum Stylus**

The third project that fell into our focus occupies a somewhat intermediate position from the point of view of potential verifiability between the polarly different approaches of the first two. Go acts as the main development language here, which is slightly less close to assembly compared to the classic C/C++ pair due to a more complex runtime with native support for light threads, but it does not rely as actively on advanced metaprogramming tools and complex typing as Rust. Due to the apparently greater expressiveness of the concurrent style of goroutines when handling network interactions, the Nitro core is even more compact than Stellar, with approximately 200k lines together with the Rust glue. It is necessary, however, to note that the central advantage of this platform, which is seamless interoperability with the Ethereum network, is achieved by including a full-scale implementation of the corresponding protocol in Go into the project, adding another 400k lines to the scale.

It should also be understood that when evaluating the complexity of formal verification at the level of executable binaries, lines are not all equal. The expressive power of Go is unfortunately not free. The runtime infrastructure providing native concurrency not only becomes an additional target for verification with fairly complex semantics but also creates an additional layer of abstraction between the source code and the assembly listing, making their direct comparison difficult.

Another architectural decision that somewhat complicates verification is the choice of the EVM distributed ledger as the persistence model. The motivation here is quite clear since interoperability with the most popular smart contract platform at the moment is obviously a huge advantage. Alas, this cryptographic ledger construction is popular due to historical reasons rather than architectural merits because it mixes the contract business logic and the consensus achievement mechanism into an inseparable whole. To notice how much this complicates everything, it is enough to look for comparison at the implementation of persistence in Stellar without regard for legacy. There, a contract completely isolated with its own data is not concerned at all with issues of indexing its state in a global trie. With this approach, business logic is strictly separated from all infrastructure issues at the level of WebAssembly isolation, which greatly facilitates both its specification and verification.

In light of the above, full verification of the Arbitrum runtime from a distance appears to be a more difficult task compared to a similar undertaking for Stellar, but one can hope it is still within the bounds of the possible.

# **Focusing on the Main Point**

Let us return from the heavens to the earth and talk about the near perspective. For quite understandable reasons, it is rash to immediately try to apply tools built on truly innovative principles to the construction of palaces. Common sense suggests first practicing on small cottages. Even the most favorable of the objects considered, the Stellar core, is no smaller than the Hermitage in this allegory. The question inevitably arises as to how to narrow the plan of work to obtain a smoother entry curve in terms of scale. The answer suggests itself because it is well known that in the metric of total financial damage, vulnerabilities in contract business logic totally dominate vulnerabilities in the blockchain platforms themselves. So why not put aside infrastructure reliability issues for now and try to concentrate on business logic alone?

By the very nature of DeFi, the business logic of each blockchain must inevitably break down into strictly isolated atomic transactions with strictly defined interaction semantics corresponding for the most part to simple accounting operations. The contracts serving these transactions can thus act as a natural training target for testing new verification technologies. When viewed from this perspective, the overall picture changes only in scale:

1. Polkadot Substrate, which does not isolate business logic from infrastructure in any way, looks like the most difficult target by a wide margin. Accounting pallets differ from system ones only in meaning, and after assembly into a single monolith, one should not even try to cover them with a formal specification. Obtaining any worth-while reliability guarantees in such a situation is possible only by limiting oneself to the verification of Ink\! contracts written in a completely self-sufficient manner, relying only on a strictly limited subset of system pallets. This does not entirely remove the problem of interleaving accounting operations with infrastructure, and such an approach can hardly be called a conventional way of using Substrate, but alas, it currently looks like the only promising one from the point of view of formal business logic verification.  
2. Stellar Soroban again looks like the benchmark of verifiability. Business logic and infrastructure are strictly separated along the physical boundaries of the WebAssembly sandbox, trans-boundary interfaces are simple to the point of absurdity, and the consensus mechanism is completely abstracted. The executable module of the contract consists only of business logic with literally zero overhead.  
3. Arbitrum Stylus is again somewhere in the middle. They clearly tried honestly to get rid of unnecessary overhead, but unfortunately, the EVM persistence model sets an insurmountable lower bar for complexity.

In this formulation, the scale of the venture stops being intimidating. The relative compactness and isolation of the objects under consideration allow for experimenting with approaches without fear of getting bogged down in details. Work with Soroban contracts can begin immediately after obtaining a functioning MVP, and even Polkadot moves from the category of unthinkable to the category of difficult but possible.

# **The Silver Bullet**

So how is this unusual new tool arranged, the capabilities of which we expect to demonstrate soon on the smart contract verification testing ground? It consists of three main components:

1. The infc compiler for the innovative high-level language Inference, combining the semantic transparency of C/C++, a Rust-like syntax more familiar to modern developers, and a unique ingredient which is managed non-determinism combined with quantifiers for the generalization of statements about computations. This language will allow for the formulation in a uniform syntax of both the source code of the contract itself and its full-scale formal specification with the expressive power of higher-order logics.  
2. The mechanisation of the WebAssembly standard in the Rocq interactive theorem solver, enhanced by a new formalism which is a definitive interpreter proven isomorphic to the inductive definition of operational semantics. Using such an interpreter as a basic tactic will effectively turn the interactive Rocq mode into a full-fledged environment for symbolic computations in WebAssembly semantics.  
3. Mathematical theory, also mechanized in Rocq, allowing for the generalization of any definitive interpreter into an entire language of tactics. With its help, proofs of properties formulated in the specifications of our new paradigm can be produced with an unprecedented level of comfort, completely abstracting away from all non-essential details of the analyzed code.

One hopes that by testing these innovations in the smart contract sandbox, in the future we will be able to scale our approach to increase the reliability of the platforms themselves. No fundamental obstacles to this are currently observed, in any case.