+++
title = "類似性ベクトル検索の理解：現代AIシステムにおけるアルゴリズムと応用"
date = 2025-02-06T13:50:00+09:00
draft = false
math = "katex"
tags = ["LLM", "Geometry", "Vector Search"]
summary = "類似性ベクトル検索の理解：現代AIシステムにおけるアルゴリズムと応用"
aliases = ["blog/similar-vectors-search"]
+++

**目次**

- [はじめに](#はじめに)
- [類似性検索データベースのアルゴリズム](#類似性検索データベースのアルゴリズム)
  - [L1距離](#l1距離)
  - [L2距離](#l2距離)
  - [内積](#内積)
  - [コサイン距離](#コサイン距離)
  - [ハミング距離](#ハミング距離)
  - [ジャカード距離](#ジャカード距離)
- [近似最近傍探索のためのグラフベースおよび量子化手法](#近似最近傍探索のためのグラフベースおよび量子化手法)
  - [局所感度ハッシュ（LSH）](#局所感度ハッシュlsh)
  - [プロダクト量子化（PQ）](#プロダクト量子化pq)
    - [プロダクト量子化の疑似コード](#プロダクト量子化の疑似コード)
    - [Lloydのアルゴリズム（別名：ボロノイ反復法）](#lloydのアルゴリズム別名ボロノイ反復法)
    - [Lloydのアルゴリズムの疑似コード](#lloydのアルゴリズムの疑似コード)
  - [階層型ナビゲート可能スモールワールド (HNSW)](#階層型ナビゲート可能スモールワールド-hnsw)
    - [HNSWのASCIIダイアグラム](#hnswのasciiダイアグラム)
    - [例: 実用的な画像検索](#例-実用的な画像検索)
  - [性能向上](#性能向上)
    - [デロネー三角形分割](#デロネー三角形分割)
- [現代AIシステムにおける類似性検索の重要性](#現代aiシステムにおける類似性検索の重要性)
    - [2次元デロネー三角形分割の疑似コード](#2次元デロネー三角形分割の疑似コード)
- [実世界の例: eコマースにおける画像検索](#実世界の例-eコマースにおける画像検索)
- [結論](#結論)

## はじめに

類似性ベクトル検索は、多くの現代的なAIシステムの根幹技術である。大規模言語モデル（LLM）や検索強化生成（RAG）、レコメンデーションエンジン、自律エージェントなど、さまざまな応用分野において中心的な役割を果たしている。データを高次元ベクトルに変換することで、システムは各アイテムの特徴や意味に基づいて「近接性」を計算できる。FAISS、Annoy、Milvus、Weaviate、Pineconeといった先進的なベクトルデータベースは、迅速かつ正確な最近傍探索を実現するために特殊なアルゴリズムを活用している。本稿では、一般的な類似性検索アルゴリズムを検討し、基礎となる距離尺度の数式的定式化および実用的応用例（画像検索や空間解析など）を通じ、これらの技術の適用方法を示す。

## 類似性検索データベースのアルゴリズム

ベクトルデータベースは通常、L1距離、L2距離、内積、コサイン距離など、複数のベクトル類似性探索アルゴリズムに基づく検索オプションを提供する。本節では、代表的な距離尺度および類似性測度について、その数式的定式化と実用例を詳述する。

### L1距離

**L1距離**（またはマンハッタン距離）は、対応するベクトル要素間の絶対差の総和を計算する。ベクトル間の絶対的な変化を測定する際に用いられ、高次元かつ疎なデータに対しては特に有効である。

*例:* 各次元がキーワードの出現頻度を表すテキストデータのレコメンデーションシステムでは、L1距離は頻度カウント全体の変化を捉えるため有用である。

数式的には、

$$
\sum_{i=1}^{n}\left| x_i - y_i \right|
$$

と表される。

### L2距離

**L2距離**（ユークリッド距離）は、ユークリッド空間内の2点（またはベクトル）間の「直線距離」を計算する。ベクトルの大きさが類似性において重要な場合に適しており、大きな差異に対してより高いペナルティを課すため、外れ値に敏感である。

*例:* 画像処理において、画素の強度差が重要な場合、L2距離は画像埋め込みの比較にしばしば用いられる。

数式的には、

$$
\sqrt{\sum_{i=1}^{n}\left( x_i - y_i \right)^2}
$$

（ここで $x$ と $y$ はベクトル）と表される。

### 内積

**内積**は、2つのベクトルがどの程度同方向を向いているかを評価する指標である。ベクトルが同一方向を向いている場合、内積は最大値を示す。なお、検索時にはコサイン距離と併用するために正規化されることが多い。

*例:* 文書の埋め込みを比較する際、内積は2つの文書が共通のテーマやトピックを持つか否かを示す。

数式的には、

$$
\sum_{i=1}^{n} x_i \cdot y_i
$$

と表される。

### コサイン距離

**コサイン距離**は、2つのベクトル間の角度の余弦を測定することで、その方向性を評価する。これはベクトルの大きさに依存せず、文書やテキストの検索において、単語の分布など方向性が重要視される場合に広く用いられる。L2距離とは異なり、ベクトルの大きさを無視し、純粋に方向性に焦点を当てる。

*例:* 自然言語処理（NLP）の分野では、文の長さが異なっていても、単語分布が類似している場合、高いコサイン類似度が得られる。

数式的には、

$$
\text{Cosine Similarity}(x,y)=\frac{\sum_{i=1}^{n}x_i \cdot y_i}{\sqrt{\sum_{i=1}^{n}x_i^2} \cdot \sqrt{\sum_{i=1}^{n}y_i^2}}
$$

$$
\text{Cosine Distance}(x,y)=1-\text{similarity}
$$

と表される。

### ハミング距離

**ハミング距離**は、2つのベクトルの各位置の値を比較し、相違点の数をカウントする。各位置が一致すれば距離は `0`、異なれば `1` とする。機械学習においては、バイナリ特徴ベクトル間の非類似性を測定するために用いられる。

*例:* ハッシュ化された画像特徴など、バイナリ表現のクラスタリングにおいて、ハミング距離はビットパターンの違いを迅速に検出する。

数式的には、

$$
d(x,y) = \sum_{i=1}^{n} \mathbf{1}(x_i \neq y_i)
$$

と表される（ここで $\mathbf{1}(\cdot)$ は指示関数である）。

### ジャカード距離

**ジャカード距離**は、文書中のトークンやキーワードなどの集合同士を比較する指標である。これは、2つの集合の積集合と和集合の比率を測定するジャカード類似度に基づいている。

*例:* 文書間のユニークな単語集合を比較することで、内容の重なりを反映したジャカード距離が得られる。

数式的には、

- ジャカード類似度:  
  $$
  J(A,B) = \frac{|A \cap B|}{|A \cup B|}
  $$
- ジャカード距離:  
  $$
  J(A,B) = 1 - J(A,B)
  $$

と表される。

## 近似最近傍探索のためのグラフベースおよび量子化手法

数百万の高次元ベクトルを扱う際、厳密な探索は非常に時間がかかる。これを克服するため、近似最近傍探索（ANN）アルゴリズムは、わずかな精度低下を許容する代わりに大幅な速度向上を実現する。以下では、局所感度ハッシュ（LSH）やプロダクト量子化（PQ）といった主要手法について、その仕組みや具体例を示す。

### 局所感度ハッシュ（LSH）

**局所感度ハッシュ（LSH）**は、類似しているが完全には一致しないデータを取得するためのファジーハッシュ技術（類似性ハッシュとも呼ばれる）である。入力データを確率的に同一の「バケット」にハッシュすることで、類似性の高いデータ同士をグループ化する。

*例:* 大規模な音楽レコメンデーションシステムでは、LSHを用いることで、類似した音響特徴を持つ楽曲を迅速にグループ化でき、クエリとして与えられた楽曲がカタログ全体と比較されることなく、類似楽曲を効率的に抽出できる。

**動作原理:**  
有限個の関数族 $F$（各関数 $h: M \rightarrow S$）が、以下の条件を満たす場合、LSH族と定義される。

- $M$ はメトリック空間 \((M,d)\)（集合と、集合内の要素間の距離を測る関数を持つ集合）である  
  - $M$：集合  
  - $d$：距離関数
- $r > 0$：閾値  
- $c > 0$：近似係数  
- $p_1 > p_2$：確率  
  - もし $d(a,b) \leq r$ ならば、$h(a) = h(b)$（すなわち、$a$ と $b$ は**少なくとも** $p_1$ の確率で衝突する）  
  - もし $d(a,b) \geq cr$ ならば、$h(a) = h(b)$（すなわち、$a$ と $b$ は**最大でも** $p_2$ の確率で衝突する）

このような関数族 $F$ を \((r, cr, p_1, p_2)\)-センシティブと呼ぶ。

**手法例:**
- ハミング距離に対するビットサンプリング  
- ジャカード指数に基づくLSHスキーム  
  $Pr[h(a) = h(b)] = \frac{|A \cap B|}{|A \cup B|}$

この手法は、非常に高次元のバイナリまたは疎なデータを扱う場合に特に効果的である。

### プロダクト量子化（PQ）

**PQ**の基本的な考え方は、高次元ベクトルを低次元の部分空間に分割することで、各部分空間が元のベクトルの複数の次元に対応するようにすることである。

$$
V^M \rightarrow \{SS\}^{M/n}
$$

ただし、$n$ は部分空間の数である。

言い換えれば、**プロダクト量子化（PQ）**は、高次元ベクトルをより小さい低次元の部分ベクトルに圧縮する手法である。各部分ベクトルは、通常 k-means によるクラスタリングで得られた最も近いセントロイドに独立して割り当てられ、量子化される。

*例:* 画像検索において、各画像が畳み込みニューラルネットワーク（CNN）によって抽出された高次元ベクトルで表現される場合、PQはセントロイドのインデックスのみを保存することでメモリ使用量を大幅に削減する。

**手順:**
1. ベクトルを $m$ 個の部分ベクトルに分割する。  
2. 各部分空間に対して k-means クラスタリングを実施し、$k$ 個のセントロイドを求める。  
3. 各部分ベクトルを、その最も近いセントロイドのインデックスに置き換える。

#### プロダクト量子化の疑似コード

```python
# プロダクト量子化 (PQ) の疑似コード
# 入力: 高次元ベクトル V, 部分空間の数 m, 各部分空間ごとのセントロイド数 k
1. V を m 個の部分ベクトルに分割する: V = [v1, v2, ..., vm]
2. 各部分ベクトル vi に対して:
    a. k-means を実行し、k 個のセントロイドを取得する。
    b. vi を最も近いセントロイドに割り当てる。
3. 各部分ベクトルに対するセントロイドのインデックスを保存する。
```

#### Lloydのアルゴリズム（別名：ボロノイ反復法）

Lloydのアルゴリズム（ボロノイ反復法とも呼ばれる）は、k-meansクラスタリングの基礎となる手法である。これは、各点を最も近いセントロイドに割り当てた後、割り当てられた点の平均値を用いてセントロイドを更新することを反復的に行い、セントロイドを洗練させる。

<iframe src="https://www.desmos.com/calculator/lhoqlfgmhi?embed" width="500" height="500" style="border: 1px solid #ccc" frameborder=0></iframe>

- 初期に $k$ 個の点（サイト）を配置する  
  - 繰り返し:
    - $k$ 個のサイトに対するボロノイ図を計算する。
    - 各ボロノイセルを積分し、そのセントロイドを計算する。
    - 各サイトを、そのボロノイセルのセントロイドに移動させる。

2次元セルにおいて、$n$ 個の三角形単体からなる場合、累積面積 $A_c = \sum_{i=0}^{n} a_i$（ここで $a_i$ は各三角形の面積）とし、新たなセルのセントロイドは

$$
C = \frac{1}{A_c}\sum_{i=0}^{n} C_i\,a_i
$$

で求められる。

全体のアルゴリズムは以下の通りである:
1. 高次元ベクトルを複数の小さい低次元の部分ベクトルに分割する。
2. 各部分ベクトルを k-means クラスタリングにより独立に量子化し、いずれかのセントロイドに割り当てる。
3. 元の完全なベクトルを保存する代わりに、**PQ**は各部分ベクトルに対して最も近いセントロイドのインデックスを保存する。
4. 検索時には、セントロイドのみを用いてベクトル間の距離を近似計算することで、処理速度が向上する。

#### Lloydのアルゴリズムの疑似コード

```python
# Lloydのアルゴリズムの疑似コード
# 入力: データ点 X, 初期セントロイド C, 反復回数 T
for t in range(1, T+1):
    for each point x in X:
        x を C の中で最も近いセントロイドに割り当てる
    for each centroid c in C:
        c を、c に割り当てられた点の平均値に更新する
```

### 階層型ナビゲート可能スモールワールド (HNSW)

**HNSW**は、各ノードが1つのベクトルを表すグラフを構築し、各ノードが固定数の最も近い近傍と接続される。グラフは複数の層に分かれており、

- **上位層:** 長距離リンクを含み、ベクトル空間内の離れた領域間のジャンプを補助する。
- **下位層:** 密な局所接続を提供し、精密な近傍探索を実現する。

*例:* 10,000枚の製品画像を含むデータセットでは、HNSWによりクエリ画像が全10,000ベクトルを評価することなく、高レベルのノードから類似画像のクラスタへ迅速に「ホップ」できる。

#### HNSWのASCIIダイアグラム

```goat
         [Vector]
            │
 ┌──────────┼──────────┐
[Vector]  [Vector]  [Vector]
            │
         [Vector]
```

```goat
vector space

[vector]────edge──connects──closest──vectors────[vector]

[vector]
                           [vector]
        [vector]
```

```goat
    fewer nodes, longer connections for long-range jumps
L ______\______
E _______\_____
V ________\____
E _____________ denser connections for local navigation
L _____________
S _____________
```

各ノードはベクトル点を表しており、各ノードは近接する最大 $m$ 個の近傍と接続される。全探索の $KNN$ 用にベクトルフィールドがインデックスされる場合、クエリは「すべての近傍」に対して実行されるが、HNSW用にインデックスされたフィールドでは、検索エンジンがグラフ上の一部のノードに対して探索を行う。

#### 例: 実用的な画像検索

例えば、あるeコマースサイトでユーザーが靴の写真をアップロードするとする。システムはその画像のベクトル埋め込みを計算し、HNSWを利用してグラフの層を迅速に移動しながら、類似した靴画像を抽出する。これにより、比較対象が数千件から数百件に削減され、迅速かつ正確なレコメンデーションが実現される。

### 性能向上

#### デロネー三角形分割

**デロネー三角形分割**は、点集合を分割して、いかなる三角形の外接円内にも他の点が存在しないようにする手法で、空間内の近傍の定義を効率的に行う方法を提供する。

平面上の与えられた点集合 $S$ に対して、任意の三角形において、$S$ の全ての点（頂点を除く）がその三角形の外接円の外側に位置するように分割される。デロネー三角形分割は、同じ点集合に対するボロノイ図と一意に対応する。

**アルゴリズム:**
1. 点集合の中央を通る水平または垂直の直線を引き、この分割により点をおおよそ $N/2$ 個ずつに分割する。後に、各グループに対してこの分割処理を再帰的に実行する。  
3. 固定された三角形分割と併せ、凸形状を形成する2組の点のセグメントを見つける。これらの点は線分で接続され、その結果得られる線分の一つを次のパスの開始点として選ぶ。ある線分上で「バブル」を内側に膨らませ、膨張する円が最初の点に到達するまで処理を行う。見つかった点を用いて、その線分上でまだ接続されていない点と接続する。得られた線分は既存の三角形分割の線分との交差がないか検査され、交差が生じた場合はその線分を削除する。その後、新たな線分を開始点として再び「バブル」を膨らませる。このサイクルは、開始線分が凸包の第二の線分と一致するまで繰り返される。

分割の計算量は $O(\log N)$、統合の計算量は $O(N)$ であり、各統合において全体の計算量は $O(N \log N)$ となる。

<iframe src="https://www.desmos.com/calculator/a9dcxauxuu?embed" width="500" height="500" style="border: 1px solid #ccc" frameborder=0></iframe>

デロネーグラフの特異な性質は、エッジによって接続される隣接点の定義にある。例えば、\(\Epsilon\)‐グラフでは、2つの点が互いに \(\Epsilon\) 以下の距離にある場合に隣接し、kNNグラフでは、ある点が他の任意の点との距離において $k$ 番目に小さい距離以内の全ての点と接続される。一方、デロネーグラフでは、各点はその空間的な隣接点すべてと隣接する。

## 現代AIシステムにおける類似性検索の重要性

類似性ベクトル検索は、さまざまなAI応用を実現するための基盤技術である。

1. **大規模言語モデル（LLM）およびRAG:**  
   LLMは、文脈を充実させるために意味的に類似した文書を取得する。たとえば、RAGシステムでは、ユーザーのクエリがベクトルに変換され、リアルタイムで類似文書が抽出されることで、生成される応答の関連性が大幅に向上する。
2. **エージェントシステム:**  
   自律エージェントは、過去の経験や関連データを迅速に取得するために類似性検索を利用し、動的環境下でより賢明な意思決定を実現する。
3. **画像検索およびレコメンデーションシステム:**  
   eコマースでは、ユーザーが製品画像をアップロードすると、システムがその画像のベクトル表現を計算し、視覚的または意味的に類似したアイテムを抽出する。これにより、検索プロセスが迅速化され、正確なレコメンデーションによって顧客満足度が向上する。
4. **スケーラビリティと効率性:**  
   データセットの規模と複雑性が増大する中、従来の探索手法では限界がある。HNSW、PQ、LSHといった先進的アルゴリズムにより、数十億の高次元ベクトルでも迅速に探索可能となり、リアルタイム処理が実現される。

*例:* 携帯電話基地局の地理座標を考える。デロネー三角形分割は、自然な隣接関係にある基地局を接続する。新たな地点に対して最寄りの基地局を探索する際、この三角形分割により近傍の基地局に迅速に絞り込むことが可能となる。

#### 2次元デロネー三角形分割の疑似コード

```python
# 分割統治法を用いたデロネー三角形分割の疑似コード
# 入力: 2次元点集合 P
1. P を x座標でソートする。
2. P を再帰的に左右 (L と R) に分割する。
3. L と R に対してデロネー三角形分割を計算する。
4. 両方の三角形分割を統合する:
    a. L と R を接続する基底エッジを特定する。
    b. デロネー条件を満たすエッジを逐次追加する。
5. 統合された三角形分割を返す。
```

## 実世界の例: eコマースにおける画像検索

例えば、ユーザーがスニーカーの画像をアップロードして類似製品を探すシナリオを考える。プロセスは以下の通りである:

- **特徴抽出:**  
  畳み込みニューラルネットワーク（CNN）が画像を高次元ベクトルに変換する。
- **インデックス作成:**  
  このベクトルは、効率的な近傍探索のためにHNSWを用い、メモリ使用量削減のためにPQが適用されたベクトルデータベースに保存される。
- **クエリ処理:**  
  ユーザーがクエリ画像を送信すると、システムはその画像のベクトルを計算し、HNSWグラフをナビゲートして最も近いベクトルを取得する。
- **結果:**  
  数ミリ秒以内に類似画像が返され、ユーザー体験が大幅に向上するとともに、購入の可能性が高まる。

## 結論

本稿では、現代のベクトルデータベースを支える主要な類似性検索アルゴリズムについて検討した。L1、L2、内積、コサイン、ハミング、ジャカードといった一般的な距離尺度を取り上げ、さらに局所感度ハッシュ、プロダクト量子化、階層型ナビゲート可能スモールワールドグラフなどの先進的な近似最近傍探索技術に踏み込んだ。また、デロネー三角形分割による性能向上についても論じ、画像検索や地理的隣接探索といった実例を通してこれらの概念を具体的に示した。

理論的基盤と実世界の応用例を融合させることにより、類似性ベクトル検索は、AIシステムにおける効率的な情報検索およびデータ解析のための重要技術として位置付けられる。データセットが拡大し、AI応用がますます文脈に即したものとなる中、先進的なベクトル検索技術は、高速・高精度かつスケーラブルなソリューションを提供する上で、今後ますます重要な役割を果たすであろう。

{{<post-socials language="jp" page_content_type="blog" telegram_post_id="11" x_post_id="1774985590670573849">}}
{{<ai-translated>}}
